{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4823ba31-d83d-4039-a702-1b74bb1d470c",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.  \n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Vision NIMs for Structured Text Extraction Workshop\n",
    "\n",
    "NVIDIA Inference Microservices (NIMs) are a collection of easy to use API driven microservices to interact with AI models.\n",
    "\n",
    "This workshop will focus on combining Florence, OCDRNet, VLMs and LLMs to build a robust structured text extraction pipeline. It is often a challenge to extract specific pieces of information from documents such Photo IDs. With many different formats of Photo IDs and irregular placement of key information it can be difficult to use traditional CV models to robustly extract fields from the Photo ID such as First Name, Last Name, Date of Birth etc.\n",
    "\n",
    "This notebook will show how to build a robust text extraction pipeline where the user can specify in natural language what fields to extract from a given image and receive the filled out fields in JSON format. To demonstrate this pipeline, the notebook will use synthetically generated Photo ID images. \n",
    "\n",
    "The Pipeline will be built using NVIDIA NIMs, which allows access to powerfull generative AI models through REST APIs to build this the pipeline. \n",
    "\n",
    "To learn more about NIMs visit <a href=https://build.nvidia.com/explore/discover> ai.nvidia.com </a>\n",
    "\n",
    "![semantic search architecture diagram](readme_assets/text_extract_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7737c7-c90a-4327-895a-38f1a22d63ae",
   "metadata": {},
   "source": [
    "This workshop has four parts:\n",
    "\n",
    "**Part 0**: Setup Environment  \n",
    "**Part 1**: Preview Dataset  \n",
    "**Part 2**: VLM Text Extraction   \n",
    "**Part 3**: Optical Character Detection and Recognition  \n",
    "**Part 4**: Structured Text Extraction Pipeline  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee548f7-e911-4c9f-ab18-83ab054b9459",
   "metadata": {},
   "source": [
    "# Part 0: Setup Environment\n",
    "\n",
    "***In the following cell, past your NIM API key*** \"nvapi-****\" to set the ```api_key ``` variable. Then continue running the cells to install the dependecies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bf7e8-e593-4364-82b3-ef56d387167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"nvapi-***\" #FIX ME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10761216-66b3-4591-8d7f-6dd874e8189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependecies\n",
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fa49f-6bcb-47c1-be30-2dfc6967bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import json\n",
    "import math \n",
    "from pathlib import Path \n",
    "from random import sample \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f07f1e-4cd7-450e-9f28-adc91eae581c",
   "metadata": {},
   "source": [
    "# Part 1: Preview Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ee3bd-4314-4f49-a38b-9c51ee9d642c",
   "metadata": {},
   "source": [
    "This notebook uses a subset of the [Synthetic dataset of ID and Travel Document (SIDTD) dataset](https://tc11.cvc.uab.es/datasets/SIDTD_1) included in the repository. The dataset is licensed under Creative Commons Attribution-ShareAlike 3.0 Unported License.\n",
    "\n",
    "The full dataset contains a collection of synthetically generated Photo IDs from various countries and includes annotations for all text in the image. Run the cells below to preview the images from the dataset and the associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f5f5b-5134-42b5-9b9d-91a4a17759fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image_folder, num_images):\n",
    "    image_files = os.listdir(image_folder)\n",
    "    sample_image_paths = sample(image_files, num_images)\n",
    "    image_paths = [Path(image_folder)/x for x in sample_image_paths]\n",
    "    grid_size = (math.ceil(num_images/3), 3)\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(15,15))\n",
    "    fig.subplots_adjust()\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "        if i >= num_images:\n",
    "            break \n",
    "        img = Image.open(image_paths[i])\n",
    "        ax.imshow(img)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522edbb-c1d1-48d6-8e25-435ed409f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"sample_data/images\"\n",
    "image_files = os.listdir(image_folder)\n",
    "image_paths = [Path(image_folder)/x for x in image_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54e2e0-2ea7-4bc4-beb0-2bc496045732",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(image_folder, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052fdc18-6aed-48e7-b197-51d0f862c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load field data\n",
    "esp_id = \"sample_data/esp_id.json\"\n",
    "with open(esp_id, \"r\")  as file:\n",
    "    esp_id_truth = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7cee8-8574-4aee-9832-92129955ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fields(image_name, json_data):\n",
    "    \"\"\"Get fields from annotations file given image name\"\"\"\n",
    "    id_list = esp_id_truth[\"_via_image_id_list\"]\n",
    "    image_index = int(image_name.split(\".\")[0].split(\"_\")[2])\n",
    "    image_id = id_list[image_index]\n",
    "    \n",
    "    regions = json_data[\"_via_img_metadata\"][image_id][\"regions\"]\n",
    "    fields = []\n",
    "    for region in regions:\n",
    "        field = region[\"region_attributes\"][\"field_name\"]\n",
    "        value = region[\"region_attributes\"][\"value\"]\n",
    "        fields.append({\"field\":field, \"value\":value})\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c3d49-b244-49eb-8cdb-398e81df6081",
   "metadata": {},
   "source": [
    "From the annotations file, we can view the associated metadata with each Photo ID. This is what we want to extract from the image of the Photo ID using AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0303cd0-7864-4835-9407-7eaeaabba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = get_fields(\"esp_id_84.jpg\", esp_id_truth)\n",
    "for x in fields:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f98e-d7c6-49dc-8b74-ad99f0dac349",
   "metadata": {},
   "source": [
    "The next sections will show how to apply various Vision AI NIMs to extract these fields from the images. \n",
    "\n",
    "Several Vision NIMs are available to help us do this:\n",
    "\n",
    "- [Visual Language Models (VLMs)](https://build.nvidia.com/microsoft/microsoft-florence-2)\n",
    "- [Florence](https://build.nvidia.com/microsoft/microsoft-florence-2)\n",
    "- [OCDRNet](https://build.nvidia.com/nvidia/ocdrnet)\n",
    "\n",
    "To view all available Vision NIMs, view [this page](https://build.nvidia.com/explore/vision). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf038a-bc2f-4e00-b552-98602aedbe8e",
   "metadata": {},
   "source": [
    "# Part 2: VLM Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56620f6-f4b3-4fda-b741-3aedef616fca",
   "metadata": {},
   "source": [
    "VLMs are capable of taking in natural language text prompts and images. By using a VLM, we can build a pipeline that is customizable and can be prompt tuned to work on difference use cases. \n",
    "\n",
    "The VLM can be provided the image of the Photo ID and a prompt with a list of fields to find and extract. VLMs are also capable of performing OCDR on their own to varying levels of success allowing it to find names, dates, ID numbers etc. \n",
    "\n",
    "The following cell will set the fields we want the VLM to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9451529-11a0-4cda-91fb-7fe2b00c82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"name\", \"surname\", \"issue date\", \"nationality\", \"gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9519e4-9947-4682-9cf0-b0084042040d",
   "metadata": {},
   "source": [
    "To use the VLM NIM, a wrapper class has been implemented. View the vlm.py file in the same directory as this notebook to view the full code. For the rest of this notebook, the NEVA 22b VLM NIM will be used. However, this can be adjusted to any of the following VLMs by changing the input link. \n",
    "\n",
    "- \"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\"\n",
    "- \"https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2\"\n",
    "- \"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\"\n",
    "- \"https://ai.api.nvidia.com/v1/vlm/google/paligemma\"\n",
    "- \"https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc88243-621a-4cd0-bd77-5d4d1b19318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm import VLM \n",
    "vlm = VLM(\"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\", api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a92d7c-98d3-4a5d-97ba-31af29c55467",
   "metadata": {},
   "source": [
    "To make the VLM extract the fields, it needs some background information in the system prompt. This will tell the VLM that its goal is to extract the supplied fields from the input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee06a6-8e09-42ba-85b3-b23401b41672",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Your job is to inspect an image and fill out a form provided by the user. This form will be provided in JSON format and will include a list of fields and field descriptions. Inspect the image and do your best to fill out the fields in JSON format based on image. Only find the provided fields. Do not add anything extra.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168132e-11d2-4999-aee3-de44a98f2644",
   "metadata": {},
   "source": [
    "The user prompt can then be a template with the fields that need to be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19950dd4-820f-4f16-8e29-cb200afec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"Here are the fields: {fields}. Fill out each field based on the image and respond in JSON format.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405ff34-0390-4ca5-ad7b-cb15befd98f8",
   "metadata": {},
   "source": [
    "Now the system prompt, user prompt and Photo ID image can be given to the VLM to extract the fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e333a-f6d7-4c29-8631-e6a31da31e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_paths[0]\n",
    "print(f\"Image File: {image}\")\n",
    "response = vlm(user_prompt, image, system_prompt=system_prompt)\n",
    "\n",
    "#Show image\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"{image}\")\n",
    "plt.imshow(Image.open(image))\n",
    "\n",
    "#Show VLM response \n",
    "print(f\"VLM Response \\n{response}\")\n",
    "\n",
    "#Show labelled fields \n",
    "labelled_fields = get_fields(Path(image).name, esp_id_truth)\n",
    "print(\"Labelled Fields\\n\")\n",
    "for x in labelled_fields:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6cf43-2018-4ba1-9d5a-e0fb6383f758",
   "metadata": {},
   "source": [
    "The output from the VLM may not be accurate. Depending on the VLM model, it can misread some characters from the ID and output mispelled fields. An option to detect the characters more accurately is to use a more powerful model built for optical character detection and recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf6426-e920-40d4-95d2-dd94a0e76e55",
   "metadata": {},
   "source": [
    "# Part 3: Optical Character Detection and Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580496bf-e828-4643-a63c-b22fad9a0adc",
   "metadata": {},
   "source": [
    "While VLMs are capable of some OCDR, a dedicated OCDR model will often perform better. Two Vision NIMs can be used for this:\n",
    "\n",
    "- [Florence](https://build.nvidia.com/microsoft/microsoft-florence-2)\n",
    "- [OCDRNet](https://build.nvidia.com/nvidia/ocdrnet)\n",
    "\n",
    "To more easily use OCDRNet and Florence, wrapper classes are provided. To view the full code, look at the florence.py and ocdrnet.py scripts in the same folder as this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411a742-98e8-435c-b740-90fa0b5a9702",
   "metadata": {},
   "source": [
    "## Part 3.1 OCDRNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46bf47-f98a-4e3e-b4c7-93b5392aac71",
   "metadata": {},
   "source": [
    "With the OCDRNet NIM, we can extract the text from the image more accurately than with a VLM alone. However, the output of this model is just raw text that has been detected in the image. It is difficult to piece together to get structured outputs. To overcome this we can provide the extracted text to a VLM or LLM for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa1997-acce-4138-8a06-038509752bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocdrnet import OCDRNET\n",
    "ocdrnet = OCDRNET(api_key)\n",
    "ocd_response = ocdrnet(image)\n",
    "ocd_response = [x[\"label\"] for x in ocd_response[\"metadata\"]]\n",
    "ocd_response = \" \".join(ocd_response)\n",
    "print(ocd_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d85ef-a4fb-4796-aafd-1ce940618d01",
   "metadata": {},
   "source": [
    "## Part 3.2 Florence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec721e7f-909f-42d6-86c7-ff394101143f",
   "metadata": {},
   "source": [
    "Florence is a very powerful and small model capable of several vision tasks such as OCDR, detection, captioning and segmentation. We can also use Florence to extract the text. Lets see how it compares to OCDRNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942b7fc-0341-4df0-9ed2-99074d5f2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from florence import Florence\n",
    "florence = Florence(api_key)\n",
    "ocd_response = florence(12, image) #12 is the task ID for OCR. Other IDs will change the task to detection, captioning etc. \n",
    "ocd_response = ocd_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(ocd_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f79c4-1606-4ca4-9469-1c578efa3f2e",
   "metadata": {},
   "source": [
    "## Part 3.3 LLM Post Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a116f3-0cbc-4c76-b368-a09e65da5462",
   "metadata": {},
   "source": [
    "The output from Florence and OCDRNet is not structured. This makes it difficult to extract specific fields. To solve these problems, we can use an LLM NIM to post process the string returned by OCDRNet or Florence. LLMs are very good at taking raw text and reformatting it. \n",
    "\n",
    "To accesss the NVIDIA LLM NIMs, the OpenAi library can be used and pointed to the LLM NIM APIs.  \n",
    "\n",
    "The LLM can be provided a prompt with that includes the OCDR results, the fields we want to extract and instructions stating to output it in JSON format. If the fields are output in JSON format then it will be easier to integrate with other services. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a06d55-ddae-4620-bdff-bf19a6b49b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"https://integrate.api.nvidia.com/v1\", api_key=api_key)\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"I have a text string that may or may not be formatted in proper json. The response should have the following keys: [{[x for x in fields]}]. Please parse the response and match the key pair values and format it in JSON. Here is the response: {ocd_response}\",\n",
    "        }\n",
    "    ]\n",
    "print(messages)\n",
    "completion = client.chat.completions.create(\n",
    "        model=\"nv-mistralai/mistral-nemo-12b-instruct\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=1024,\n",
    "        stream=False,\n",
    "    )\n",
    "print(completion)\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cc458-3939-487f-a0d7-6ec8529bf9b4",
   "metadata": {},
   "source": [
    "From the output, you can see the LLM was able to take the raw text string and reformat it into JSON output with the specified fields. However, there are still some errors. To get better accuracy the VLM, OCDR model and LLM can all be combined into one pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d1a0c-cf47-4aec-ade2-413e9ba86a2c",
   "metadata": {},
   "source": [
    "# Part 4: Text Extraction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c118f1-2ffc-4f47-965e-ec1bd2ed2568",
   "metadata": {},
   "source": [
    "With these three pieces, we can build a full pipeline to combine a VLM, OCDR model and LLM to extract structured text from any image. \n",
    "\n",
    "The OCDR model (Florence or OCDRNet) will extract all characters from the image and pass it to either the VLM or LLM. \n",
    "The VLM will attempt to extract fields from the image and output the fields in JSON format. \n",
    "The LLM will take the output from either the VLM or OCD model and ensure it is in the proper JSON format. \n",
    "\n",
    "![Pipeline Diagram](readme_assets/notebook_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d37ecd-236e-46b2-b53d-1ae430a84989",
   "metadata": {},
   "source": [
    "## Part 4.1: Text Extraction Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdca343-ba3e-4bd2-8152-99c1f1fc0524",
   "metadata": {},
   "source": [
    "Below is the code to piece together the different models to form the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dad03b-b5f1-45c9-8965-276bde17f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm import VLM\n",
    "from florence import Florence\n",
    "from ocdrnet import OCDRNET\n",
    "from openai import OpenAI\n",
    "\n",
    "class TextExtraction:\n",
    "\n",
    "    def __init__(self, api_key, vlm=None, llm=None, ocd=None, **kwargs):\n",
    "        self.api_key = api_key\n",
    "        self.vlm = vlm  # [\"nvidia/neva-22b\"]\n",
    "        self.llm = llm  # [\"nv-mistralai/mistral-nemo-12b-instruct\"]\n",
    "        self.ocd = ocd  # [\"nvidia/ocdrnet\", \"microsoft/florence-2\"]\n",
    "\n",
    "        # VLM or LLM and OCD required\n",
    "        if not self.vlm:\n",
    "            if not (self.ocd and self.llm):\n",
    "                raise Exception(\"VLM or OCD and LLM required.\")\n",
    "\n",
    "        self.vlm_system_prompt = kwargs.get(\n",
    "            \"vlm_system_prompt\",\n",
    "            \"Your job is to inspect an image and fill out a form provided by the user. This form will be provided in JSON format and will include a list of fields and field descriptions. Inspect the image and do your best to fill out the fields in JSON format based on image. The JSON output should be in a JSON code block.\",\n",
    "        )\n",
    "        self.llm_system_prompt = kwargs.get(\n",
    "            \"llm_system_prompt\",\n",
    "            \"You are an AI assistant whose job is to inspect a string that may have json formatted output. This json format may not be correct so you must extract the json and make it properly formatted in a JSON block. You will be provided a list of keys that you must find in the input string. If you cannot find the associated value then put an empty string.\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, image, field_names, field_descriptions=None):\n",
    "        \"\"\"image - PIL image or file path\"\"\"\n",
    "\n",
    "        field_descriptions = (\n",
    "            [\"\"] * len(field_names)\n",
    "            if field_descriptions is None\n",
    "            else field_descriptions\n",
    "        )\n",
    "\n",
    "        # Get Field Names and Descriptions in dict\n",
    "        fields = {}\n",
    "        for x in range(len(field_names)):\n",
    "            fields[field_names[x]] = field_descriptions[x]\n",
    "\n",
    "        # Stage 1: OCDR with OCDRNet or Florence\n",
    "        if self.ocd is not None:\n",
    "            # Setup OCD\n",
    "            if self.ocd == \"microsoft/florence-2\":\n",
    "                florence = Florence(self.api_key)\n",
    "                ocd_response = florence(12, image)\n",
    "                ocd_response = ocd_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            elif self.ocd == \"nvidia/ocdrnet\":\n",
    "                ocdrnet = OCDRNET(self.api_key)\n",
    "                ocd_response = ocdrnet(image)\n",
    "                ocd_response = [x[\"label\"] for x in ocd_response[\"metadata\"]]\n",
    "                ocd_response = \" \".join(ocd_response)\n",
    "        else:\n",
    "            ocd_response = None\n",
    "\n",
    "        # Stage 2: VLM Field Extraction\n",
    "        if self.vlm is not None:\n",
    "            # setup VLM\n",
    "            vlm = VLM(f\"https://ai.api.nvidia.com/v1/vlm/{self.vlm}\", self.api_key)\n",
    "\n",
    "            # Form Prompt\n",
    "            user_prompt = f\"Here are the fields: {fields}. Fill out each field based on the image and respond in JSON format.\"\n",
    "            if ocd_response:\n",
    "                user_prompt = (\n",
    "                    user_prompt\n",
    "                    + f\"To assist you with filling out the fields. The following text has been extract from the image: {ocd_response}\"  # add OCDR output if available\n",
    "                )\n",
    "            vlm_response = vlm(user_prompt, image, system_prompt=self.vlm_system_prompt)\n",
    "        else:\n",
    "            vlm_response = None\n",
    "\n",
    "        # Stage 3: LLM Post Processing\n",
    "        if self.llm:\n",
    "            llm_input = vlm_response if vlm_response else ocd_response\n",
    "            # LLM call for fixing json formatting\n",
    "            client = OpenAI(\n",
    "                base_url=\"https://integrate.api.nvidia.com/v1\", api_key=self.api_key\n",
    "            )\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.llm_system_prompt},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"I have a text string that may or may not be formatted in proper json. The response should have the following keys: [{[x for x in fields.keys()]}]. Please parse the response and match the key pair values and format it in JSON. Here is the response: {llm_input}\",\n",
    "                },\n",
    "            ]\n",
    "            completion = client.chat.completions.create(\n",
    "                model=self.llm,\n",
    "                messages=messages,\n",
    "                temperature=0.2,\n",
    "                top_p=0.7,\n",
    "                max_tokens=1024,\n",
    "                stream=False,\n",
    "            )\n",
    "\n",
    "            llm_response = completion.choices[0].message.content\n",
    "        else:\n",
    "            llm_response = None\n",
    "\n",
    "        final_response = llm_response if llm_response else vlm_response\n",
    "\n",
    "        # Extract the JSON part from the code block\n",
    "        try:\n",
    "            #Try to find json code block\n",
    "            re_search = re.search(\n",
    "                r\"```json\\n(.*?)\\n```\", final_response, re.DOTALL\n",
    "            )\n",
    "            if re_search:\n",
    "                json_string = re_search.group(1)\n",
    "            #If no code block then find curly braces \n",
    "            else:\n",
    "                left_index = final_response.find(\"{\")\n",
    "                right_index = final_response.rfind(\"}\")\n",
    "                json_string = final_response[left_index:right_index+1]\n",
    "                \n",
    "            json_object = json.loads(json_string)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON Parsing Error: {e}\")\n",
    "            return {key:None for key in field_names} #return empty expected dict with no values \n",
    "\n",
    "        return json_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f57f74-ace3-4d99-87df-a037c550e97a",
   "metadata": {},
   "source": [
    "## Part 4.2: Building and Testing The Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fa730-2518-4970-98f1-9363edc82885",
   "metadata": {},
   "source": [
    "The pipeline code is adaptable such that each model can be included or exluded. \n",
    "This allows us to build four combinations. \n",
    "\n",
    "- VLM\n",
    "- OCD + VLM \n",
    "- OCD + LLM\n",
    "- OCD + VLM + LLM\n",
    "\n",
    "Run the cells below to execute the pipelines on a sample Photo ID image and compare the results. Experiment with different VLMs, LLMs and ocd models. The valid OCDR, VLM and LLM parameter inputs are listed below:\n",
    "\n",
    "OCDR\n",
    "- \"nvidia/ocdrnet\"\n",
    "- \"microsoft/florence-2\"\n",
    "\n",
    "VLMs\n",
    "- \"nvidia/neva-22b\"\n",
    "- \"microsoft/phi-3-vision-128k-instruct\"\n",
    "- \"google/paligemma\"\n",
    "- \"adept/fuyu-8b\"\n",
    "- \"microsoft/kosmos-2\"\n",
    "\n",
    "LLMs\n",
    "- \"nv-mistralai/mistral-nemo-12b-instruct\"\n",
    "- \"mistralai/mixtral-8x22b-instruct-v0.1\"\n",
    "- \"meta/llama-3.1-8b-instruct\"\n",
    "- \"meta/llama-3.1-70b-instruct\"\n",
    "- Any other LLM listed on [this page](https://docs.api.nvidia.com/nim/reference/llm-apis) can be used. \n",
    "\n",
    "Full documentation on these models can be found on [this page](https://docs.api.nvidia.com/nim/reference/models-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525e7c0-a561-4e6b-a3f1-f922ffe6d997",
   "metadata": {},
   "source": [
    "Now we can instantiate all four pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc8b9c-5a0f-48e6-8c1d-dbef5d02e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_vlm = TextExtraction(api_key, vlm=\"nvidia/neva-22b\")\n",
    "pipeline_ocd_vlm = TextExtraction(api_key, vlm=\"nvidia/neva-22b\", ocd=\"nvidia/ocdrnet\")\n",
    "pipeline_ocd_llm = TextExtraction(api_key, llm=\"nv-mistralai/mistral-nemo-12b-instruct\", ocd=\"nvidia/ocdrnet\")\n",
    "pipeline_ocd_vlm_llm = TextExtraction(api_key, vlm=\"nvidia/neva-22b\", llm=\"nv-mistralai/mistral-nemo-12b-instruct\", ocd=\"nvidia/ocdrnet\")\n",
    "pipelines = [pipeline_vlm, pipeline_ocd_vlm, pipeline_ocd_llm, pipeline_ocd_vlm_llm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c5e8f-e995-48fc-b9ea-983fd2312705",
   "metadata": {},
   "source": [
    "The following function will take the pipelines and run them on the same image and set of fields and return the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d7d21-47f7-4006-b2b2-8c9fe88323f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipelines(pipelines, fields, field_descriptions, data, image):\n",
    "    \"\"\"Test a lists of pipelines on a sample image\"\"\"\n",
    "    results = []\n",
    "    true_fields = get_fields(Path(image).name, esp_id_truth)\n",
    "    formatted_true_fields = {}\n",
    "    for tf in true_fields:\n",
    "        if tf[\"field\"] in fields:\n",
    "            formatted_true_fields[tf[\"field\"]] = tf[\"value\"]\n",
    "    results.append(formatted_true_fields)\n",
    "    for pipeline in pipelines:\n",
    "        result = pipeline(image, fields, field_descriptions=field_descriptions)\n",
    "        results.append(result)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbcf67-acfa-4b1f-96c1-7a869438d94f",
   "metadata": {},
   "source": [
    "Now we can define the fields we want to extract from the Image, run all the pipelines and review the results. You can run the following cell multiple times to see the resluts on different images. You can also adjust the field names and descriptions to control the information that pipeline extracts from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214fc53-57e9-49ab-892e-8e892824a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run pipeline to extract fields on random sample image \n",
    "fields = [\"birth_date\", \"expiry_date\", \"gender\", \"nationality\", \"name\", \"surname\"]\n",
    "field_descriptions = [\"date of birth\", \"expiration date of ID\", \"gender\", \"nationality or country of origin\", \"first name\", \"last name or surname\"]\n",
    "image = sample(image_paths, 1)[0]\n",
    "results = test_pipelines(pipelines, fields, field_descriptions, esp_id_truth, image)\n",
    "\n",
    "#print results \n",
    "df = pd.DataFrame(results)\n",
    "df.insert(0, \"Pipeline\",  [\"Truth\", \"VLM\", \"OCD+VLM\", \"OCD+LLM\", \"OCD+VLM+LLM\"]) #add identifiable names to each pipeline \n",
    "display(df.style.set_caption(\"Structured Text Extraction\").hide(axis=\"index\"))\n",
    "\n",
    "#Show image\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"{image}\")\n",
    "plt.imshow(Image.open(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca0913-96cc-45aa-b1a8-6b7e611f7882",
   "metadata": {},
   "source": [
    "## 4.3 Interactive Gradio UI for Structured Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64fbeb0-a7ce-42cf-8ffe-892d562acbb5",
   "metadata": {},
   "source": [
    "This pipeline can be wrapped in a Gradio UI to provide an easy to use interface to test structured text extraction on any image and model combinations. The UI is a great way to quickly explore new uses cases beyond just Photo IDs.  Run the cell below to launch the Gradio UI. \n",
    "\n",
    "Once launched, the UI will be available at http://localhost:7860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f6f65-96dd-4837-b1ea-f137c15cf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{python_exe} main.py {api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa6a33-9605-4708-a288-c0140e576e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
