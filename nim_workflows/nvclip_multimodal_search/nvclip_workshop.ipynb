{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b1f7c-838e-4a02-a821-38f8f5448274",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.  \n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# NV-CLIP NIM Multimodal Search Workshop\n",
    "\n",
    "NVIDIA Inference Microservices (NIMs) are a collection of easy to use API driven microservices to interact with AI models.\n",
    "\n",
    "This workshop will focus on the NV-CLIP NIM which is a commerically viable embedding model for both images and text. Having both images and text in the same embedding space makes it easy to determine how similar text is to any given image. \n",
    "\n",
    "This capability of being able to associate images with text, has led to the development of a new class of open vocabulary models that allow you to detect or classify anything based on text descriptions and powerful search applications through natural language. This notebook will explore how the embeddings produced by NV-CLIP can be used to create a semantic search application over a traffic camera dataset. \n",
    "\n",
    "To learn more about NIMs visit <a href=https://build.nvidia.com/explore/discover> ai.nvidia.com </a>\n",
    "\n",
    "![semantic search architecture diagram](readme_assets/semantic_search_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302225dd-1944-4c4b-886b-fc7674b55627",
   "metadata": {},
   "source": [
    "This workshop has four parts:\n",
    "\n",
    "**Part 0**: Setup Environment  \n",
    "**Part 1**: NV-CLIP Requests  \n",
    "**Part 2**: Embeddings & Similarity   \n",
    "**Part 3**: Traffic Vehicle Search  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec21404-0f54-4bba-a2d4-57a360add0f4",
   "metadata": {},
   "source": [
    "# Part 0: Setup Environment\n",
    "\n",
    "***In the following cell, past your NIM API key*** \"nvapi-****\" to set the ```api_key ``` variable. Then continue running the cells to install the dependecies. Note that this notebook will download a public traffic camera dataset that is 155MB and will be used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ed121-7fc1-4877-8015-34acdc7d66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"nvapi-***\" #FIX ME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b1ec9-bfd4-40cb-97e5-16a81ed7d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependecies\n",
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ff566-0f7c-4ddb-a14e-dd276bea5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure all imports work \n",
    "import requests \n",
    "import json\n",
    "import os \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image \n",
    "from random import randint \n",
    "from pathlib import Path \n",
    "from tqdm import tqdm \n",
    "from random import sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4e1c7-be25-4472-83c7-3404c0401e80",
   "metadata": {},
   "source": [
    "If there are any errors at this point, ensure the dependecies installed properly can be imported before continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddc3cc-87ce-4609-9191-eba5f8eb0ccc",
   "metadata": {},
   "source": [
    "# Part 1: NV-CLIP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443672ef-3205-48cc-8614-7c50740d0b8a",
   "metadata": {},
   "source": [
    "This section shows how to call the NV-CLIP NIM API with a POST request to get an embedding for text input. \n",
    "\n",
    "Sending a request to the NV-CLIP NIM API requires a header that includes your API key for authorization and a payload with the content to be embedded. \n",
    "\n",
    "In the header, the API key should be presented as a Bearer token and the request body is JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913b0bd-c9cb-4bc9-be66-0f1350166f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://integrate.api.nvidia.com/v1/embeddings\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Accept\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ccc88-6711-417a-af6e-d44aad1856d0",
   "metadata": {},
   "source": [
    "### Part 1.1: Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce6b09-d6aa-41bf-ba9c-482049bb8187",
   "metadata": {},
   "source": [
    "We can then form the payload to generate an embedding for text input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d2e0f-d964-4795-8293-b9a1b3326ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input\": \"An Apple\",\n",
    "    \"model\": \"nvidia/nvclip\"\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e4fc-24ba-4d35-b75b-e8a7746d3c26",
   "metadata": {},
   "source": [
    "The payload required for NV-CLIP is an \"input\" field with a value that is either a string or a list of strings and a \"model\" field that specifies using the \"nvidia/nvclip\" embedding model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e8644-05f1-434d-8986-0b0e8a5d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(base_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b33e9-4117-429e-af3a-0b59b1a2d0a0",
   "metadata": {},
   "source": [
    "The Python requests library can then be used to send a POST request to the NV-CLIP API url. The response will be in JSON format and can be parsed to get the embedding vector for our input phrase \"An Apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7be3c-8696-4d41-92b6-881e9955419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = response[\"data\"][0][\"embedding\"]\n",
    "print(len(embedding_vector))\n",
    "print(type(embedding_vector[0]))\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5542524-6a8b-49a1-9945-7598b89b452f",
   "metadata": {},
   "source": [
    "The vector that gets returned is a list of 1024 floating point values that represent our text input in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd365ff-4005-494a-bee6-a398c83f2437",
   "metadata": {},
   "source": [
    "### Part 1.2 Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3e24b-ddd9-4224-9de5-2e08f2744ee2",
   "metadata": {},
   "source": [
    "A unique property of the CLIP family of models, is that they can also be used to embed images. To send an image to the NVCLIP NIM in the POST request, it needs to be converted to a base 64 string. \n",
    "\n",
    "NV-CLIP processes images at 336x336 resolution so we can first downsize our input image before converting it to a base 64 string. Resizing the image is not necessary but we can do it to reduce our payload size.\n",
    "\n",
    "The following code cells will generate the embedding of this image of an apple. \n",
    "\n",
    "![image](test_image.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e194f0-2a8c-4a14-87e6-df3e4bc450e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64 \n",
    "def process_image(image):\n",
    "    \"\"\" Resize image, encode as jpeg to shrink size then convert to b64 for upload \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "    elif isinstance(image, Image.Image):\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    image = image.resize((336,336)) #Resize or center crop and padding to be square are common approaches \n",
    "    buf = io.BytesIO() #temporary buffer to save processed image \n",
    "    image.save(buf, format=\"JPEG\") #save as jpeg to reduce size\n",
    "    image = buf.getvalue()\n",
    "    image_b64 = base64.b64encode(image).decode() #convert to b64 string\n",
    "    assert len(image_b64) < 180_000, \"Image too large to upload.\" #ensure image is small enough\n",
    "    return image_b64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab0858-ad9c-4da3-bb65-5ba350b180f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"test_image.jpeg\"\n",
    "image_string = f\"data:image/jpeg;base64,{process_image(image_file)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb578b-512f-4d4c-9b0a-d24a255cbe1b",
   "metadata": {},
   "source": [
    "We can now add the image string the same way we added our text input in the payload of the request. When the NVCLIP NIM receives a string with this format, it will automatically load it as an image. \n",
    "\n",
    "```data:image/jpeg;base64,{b64_string}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2773e-cc01-4f25-8dc8-66431780f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"input\": [image_string], \"model\":\"nvidia/nvclip\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57fedc-ab5a-4037-9cfb-b5a5815caa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(base_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f8043-92b4-420a-bfb7-45a808c7bd95",
   "metadata": {},
   "source": [
    "In the request reponse, we get the vector representation (embedding) of the input image. The next section will show how we can compare these vectors to determine similarity.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"readme_assets/nvclip_diagram.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f90905-282e-45bc-ba54-06889893a6ea",
   "metadata": {},
   "source": [
    "## Part 2: Embeddings & Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db46abb-580c-4150-978f-c5bf2505578b",
   "metadata": {},
   "source": [
    "Part 1 showed how to use the NVCLIP API to convert image and text into an embedding. This embedding is a 1024 dimensional vector of floating point numbers. One of the main benefits of converting image and text into a vector with an embedding model, is it enables an easy way to determine how similar images and text are to each other by calculating the cosine similarity between the vectors.  NVCLIP was trained on millions of image text pairs, allowing it to learn the association betweeen text and images. This knowledge is then captured in the embeddings that NVCLIP generates allowing us to use it as a tool to determine similarity between text and images. \n",
    "\n",
    "The ability to directly compare images and text in this manner enables powerful capabilities such as zero shot classification, semantic search and is critical for Visual Language Models (VLMs) such as LLaVA. The rest of this notebook will focus on similarity and how to build a semantic search application with the NVCLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5f35d-c08f-4738-bb27-71bfdc72fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input\": [\"apple\", \"banana\", image_string],\n",
    "    \"model\": \"nvidia/nvclip\"\n",
    "}\n",
    "response = requests.post(base_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c25fd4-c0fa-40ce-b8a1-14fe4d71482a",
   "metadata": {},
   "source": [
    "The NVCLIP API allows more than one input item at a time. In our payload, we can put a list of items as input. This list can be a mix of both text and images. Lets generate embeddings for the text \"apple\", \"banana\" and an image of an apple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d72112-a4ca-4efb-aa43-f6b5496c62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_vec = response[\"data\"][0][\"embedding\"]\n",
    "banana_vec = response[\"data\"][1][\"embedding\"]\n",
    "apple_image_vec = response[\"data\"][2][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93965eb-cf92-46ab-8c70-0ae5f68aa3c2",
   "metadata": {},
   "source": [
    "These vectors that represent each of our inputs can now be compared to each other to get a measure of how similar they are too each other. This can be done by calculating the cosine similarity between two of the vectors. \n",
    "\n",
    "When using cosine similarity, you will see values between [-1, 1]. A value close to 1 means the vectors are very similar to each other. We can plot the similarity scores for our three vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c94c4-aa38-49e6-a0e1-4d3dd447b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = np.array([apple_vec, banana_vec, apple_image_vec]) #convert vectors to np arrays \n",
    "labels = [\"'apple'\", \"'banana'\", \"apple_image\"]\n",
    "cosine_sim_matrix = cosine_similarity(embedding_vectors)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cosine_sim_matrix[:, 2:3], annot=True, cmap='coolwarm', xticklabels=[labels[2]], yticklabels=labels)\n",
    "plt.title('Cosine Similarity Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cf374-17f1-4929-a7d6-44947e8537fe",
   "metadata": {},
   "source": [
    "From the heatmap, we can see that the similarity score between the image of an apple and the text \"apple\" is higher(closer to 1) than with the text \"banana\". Using NVCLIP and the similarity score between the embeddings, we were essentially able to classify the image as an apple. By adding more text labels and comparing against image embeddings, NVCLIP can be used to build a zero shot classification model that allows us to classify any images on arbitrary classes without any training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4e1e4-0fc2-410e-86e9-5b33429f0141",
   "metadata": {},
   "source": [
    "## Part 3 Traffic Vehicle Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2dee2-7676-4956-936d-27e73244f281",
   "metadata": {},
   "source": [
    "Lets apply NV-CLIP to a practical application of vehicle search over a large number of traffic cameras. \n",
    "\n",
    "It is often a challenge that there is more footage collected than possible to manually review and find what you are looking for. We can use NVCLIP to help us search through a large dataset of objects to find what we want. In this example, we will search over images of vehicles that have been cropped out of traffic cameras. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcf1dd-0cf9-45b0-8aa4-1fe704280585",
   "metadata": {},
   "source": [
    "## Part 3.1 Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46719d94-7e64-4556-b37b-08a966be087e",
   "metadata": {},
   "source": [
    "The rest of the notebook will use the [STREETS dataset hosted by the University of Illinois](https://databank.illinois.edu/datasets/IDB-3671567) which is a collection of traffic camera images.\n",
    "\n",
    "Run the following cells to download, view and prepare the traffic camera dataset for semantic search. This will download 148 MB of data.\n",
    "\n",
    "For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose.\n",
    "\n",
    "**Note**: If you are on Windows or the following cell fails to download and extract the dataset, then follow these steps to manually prepare the data:\n",
    "\n",
    "1) Download only the \"vehicleannotations.zip\" file from [this page](https://databank.illinois.edu/datasets/IDB-3671567)\n",
    "2) Place the \"vehicleannotations.zip\" in the same directory as this notebook\n",
    "3) Make a new folder in the same directory as this notebook called \"traffic_data\"\n",
    "4) In your file explorer, extract the \"vehicleannotations.zip\"\n",
    "5) Open the extracted directory and copy the \"vehicleannotations\" folder and place it into the \"traffic_data\" folder\n",
    "6) Ensure that in the same directory as your notebook you now have a folder\n",
    "\n",
    "``` \n",
    "nvclip_semantic_search/  \n",
    "├── nvclip_workshop.ipynb  \n",
    "└── traffic_data/  \n",
    "    └── vehicleannotations/  \n",
    "        ├── images  \n",
    "        └── annotations\n",
    "```     \n",
    "7) Once you verify the directory structure is correct, skip the following cell and continue with the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5eacc1-50f5-411a-885f-3f2d8a8c6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset if not downloaded. SKIP if on Windown and follow the above instructions.\n",
    "!wget -N https://databank.illinois.edu/datafiles/ht2io/download #155MB download\n",
    "!unzip -q -o download -d traffic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2767045-8346-4dd5-a138-2b0dda0de1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path(\"traffic_data/vehicleannotations\")\n",
    "image_folder = dataset_folder/\"images\"\n",
    "annotation_folder = dataset_folder/\"annotations\"\n",
    "crop_image_folder = dataset_folder/\"cropped_vehicles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400796b-8fc6-4a69-bac9-324059784017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "def plot_images(image_folder, num_images):\n",
    "    image_files = os.listdir(image_folder)\n",
    "    sample_image_paths = sample(image_files, num_images)\n",
    "    image_paths = [image_folder/x for x in sample_image_paths]\n",
    "    grid_size = (math.ceil(num_images/3), 3)\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(15,15))\n",
    "    fig.subplots_adjust()\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "        if i >= num_images:\n",
    "            break \n",
    "        img = Image.open(image_paths[i])\n",
    "        ax.imshow(img)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d622d65-bc58-4244-93b9-e7d8b087a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(image_folder, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534dd72-9a6f-441a-b9a2-6fcf988df3f7",
   "metadata": {},
   "source": [
    "This dataset includes annotations for each image with bounding boxes of each car present in the image. Using these annotations we can crop out each car from the image. In this use case we will take advantage of this detection data being provided, however this could be combined with a detection model to first find the objects of interest and generate the bounding boxes. \n",
    "\n",
    "The following cells will crop out the vehicles and display them. After cropping out the vehicles there should be around 1600 images. Each NVCLIP request can accept up to 64 items so it will take 25 requests (25 credits) to embedd all the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc9fba-85ec-475a-bb2c-0c5f7cdcca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop out cars\n",
    "annotation_file = annotation_folder/\"vehicle-annotations.json\"\n",
    "with open(annotation_file, \"r\") as file:\n",
    "    annotations = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9ec8b-2f71-4544-8638-5c2fae12a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_crop(image_path, bbox, output_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.crop(bbox)\n",
    "    image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7abcdd-d9d6-46dd-a1ad-8192703b620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(crop_image_folder, exist_ok=True)\n",
    "\n",
    "#all_bboxes = []\n",
    "for key, value in tqdm(annotations.items()):\n",
    "    \"\"\"For each annotated vehicle, crop and save it\"\"\"\n",
    "    file_name = value[\"filename\"]\n",
    "\n",
    "    for x, region in enumerate(value[\"regions\"].values()): #each region is a vehicle \n",
    "        #convert polygon annotations to a bounding box \n",
    "        x_points = region[\"shape_attributes\"][\"all_points_x\"] \n",
    "        y_points = region[\"shape_attributes\"][\"all_points_y\"]\n",
    "        bbox = [min(x_points), min(y_points), max(x_points), max(y_points)]\n",
    "        area = (bbox[2]-bbox[0]) * (bbox[3] - bbox[1])\n",
    "        if area < 10000: #skip crops that are too small \n",
    "            continue \n",
    "        #all_bboxes.append(bbox)\n",
    "        file_path = image_folder/file_name\n",
    "        save_crop(file_path, bbox, f\"traffic_data/vehicleannotations/cropped_vehicles/{str(x).zfill(3)}_{file_name}\") #save cropped out car\n",
    "\n",
    "print(f\"Cropped Vehicle Images: {len(os.listdir(crop_image_folder))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102dc80-5af5-4ff5-aceb-4228d03eb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(crop_image_folder, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b8525-e19e-4fce-9212-724ef7d06e12",
   "metadata": {},
   "source": [
    "Now that we have images that contain individual vehicles, we can use NVCLIP to generate embeddings for each one to start building our semantic search application. \n",
    "\n",
    "Instead of directly calling NV-CLIP through POST requests, we can wrap it in an easy to use Python class with multi-threading support to speed up the responses. To view the full code for this view the nvclip.py file in the same directory as this notebook. \n",
    "\n",
    "With this NV-CLIP class, we can instantiate a new object and then pass it a list of file paths to where the cropped vehicles have been stored. The class will handle the image processing and requests behind the scenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5a4e1-3f5e-468e-8e24-5d86f6d7dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvclip import NVCLIP \n",
    "nvclip = NVCLIP(api_key)\n",
    "cropped_image_files = [str(crop_image_folder/x) for x in os.listdir(crop_image_folder)] #list of all image paths to give to nvclip\n",
    "text_prompts = [\"A School Bus\"] #list of text prompts to embed and display with the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d83bb-303e-4fb1-be4e-eb4196907f5e",
   "metadata": {},
   "source": [
    "Now we can call NVCLIP on the image paths, parse the reponse and store it in a dictionary format to make it easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e96de-66dd-4f97-832c-3e2341e078d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed images\n",
    "resp = nvclip(cropped_image_files, resize=False)\n",
    "image_embedding_data = []\n",
    "for i, data in enumerate(resp[\"data\"]):\n",
    "    data = {\"id\":i, \"vector\":data[\"embedding\"], \"file_name\":cropped_image_files[i]}\n",
    "    image_embedding_data.append(data)\n",
    "    \n",
    "image_embeddings = np.array([x[\"vector\"] for x in image_embedding_data])\n",
    "image_file_names = [x[\"file_name\"] for x in image_embedding_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55274d0-c8c0-4bd1-8c6f-21798b94cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed text\n",
    "resp = nvclip(text_prompts)\n",
    "text_embedding_data = []\n",
    "for i, data in enumerate(resp[\"data\"]):\n",
    "    data = {\"id\":i, \"vector\":data[\"embedding\"], \"text_prompt\":text_prompts[i]}\n",
    "    text_embedding_data.append(data)\n",
    "\n",
    "text_embeddings = np.array([x[\"vector\"] for x in text_embedding_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c07a44-1eb6-4e9c-9b40-c4d9a9ae62d0",
   "metadata": {},
   "source": [
    "## 3.2 Plot Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad344d-5035-4fc8-a0df-201a1eda4a3e",
   "metadata": {},
   "source": [
    "Now that the image embeddings have been generated for all of the cropped vehicles, the embeddings can be projected onto a 2D plot to understand more about the embedding space and explore the clusters. \n",
    "\n",
    "The clusters in the plot should represent images and text that are similar to each other. For example cars of the same type, color and shape should be close to each other once plotted. \n",
    "\n",
    "We will also add a red dot representing the embedding for the phrase \"A School Bus\". This will allow us to identify the school bus cluster as it should appear near the images of school buses. \n",
    "\n",
    "Run the cell bellow to generate the plot. It may take a few minutes to appear after running the cell. \n",
    "\n",
    "Once it appears, hover your cursor around the plot to see how NVCLIP clusters the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5178c9-6fed-49dd-887d-0a3a95605037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, CustomJS\n",
    "from bokeh.layouts import column\n",
    "from bokeh.io import output_notebook\n",
    "from pathlib import Path \n",
    "import base64\n",
    "\n",
    "#helper to show images in the plot \n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Enable Bokeh output in Jupyter Notebooks\n",
    "output_notebook()\n",
    "\n",
    "hosted_image_paths = [f\"data:image/jpg;base64,{encode_image_to_base64(x)}\" for x in image_file_names]\n",
    "combined_embeddings = np.vstack([text_embeddings, image_embeddings])\n",
    "contents = text_prompts.copy()\n",
    "contents.extend(hosted_image_paths)\n",
    "content_names = text_prompts.copy()\n",
    "content_names.extend([str(Path(x).name) for x in image_file_names])\n",
    "\n",
    "content_types = [\"text\"]\n",
    "content_types.extend([\"image\"] * len(image_file_names))\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, early_exaggeration=30, n_iter=2000, random_state=42, metric=\"cosine\")\n",
    "embedding_2d = tsne.fit_transform(combined_embeddings)\n",
    "\n",
    "split_index = len(text_prompts)\n",
    "# Prepare text data for Bokeh\n",
    "text_source = ColumnDataSource(data=dict(\n",
    "    x=embedding_2d[0:split_index, 0],\n",
    "    y=embedding_2d[0:split_index, 1],\n",
    "    content=contents[0:split_index],\n",
    "    content_name=content_names[0:split_index],\n",
    "    content_type = content_types[0:split_index]\n",
    "))\n",
    "\n",
    "# Prepare image data for Bokeh\n",
    "image_source = ColumnDataSource(data=dict(\n",
    "    x=embedding_2d[split_index:, 0],\n",
    "    y=embedding_2d[split_index:, 1],\n",
    "    content=contents[split_index:],\n",
    "    content_name=content_names[split_index:],\n",
    "    content_type = content_types[split_index:]\n",
    "))\n",
    "\n",
    "# Create plot\n",
    "p = figure(title=\"NVCLIP Embedding Visualization\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\")\n",
    "\n",
    "image_renderer= p.scatter('x', 'y', size=8, source=image_source, color='blue', legend_label=\"Image Embedding\")\n",
    "\n",
    "# Highlight the text embedding with a different color\n",
    "text_renderer = p.scatter('x', 'y', size=10, source=text_source, color='red', legend_label='Text Embedding')\n",
    "\n",
    "#tooltip to display text\n",
    "text_hover = HoverTool(renderers=[text_renderer], tooltips=\"\"\"\n",
    "\n",
    "    <h2>@content</h2>\n",
    "    \n",
    "\"\"\")\n",
    "p.add_tools(text_hover)\n",
    "\n",
    "# tooltip to display images \n",
    "image_hover = HoverTool(renderers=[image_renderer], tooltips=\"\"\"\n",
    "    <div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 15px;\">@content_name</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <img src=\"@content\" height=\"100\" alt=\"@file_name\" style=\"float: left; margin: 0px 15px 15px 0px;\" />\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\")\n",
    "p.add_tools(image_hover)\n",
    "\n",
    "# Layout\n",
    "layout = column(p)\n",
    "\n",
    "# Show plot in notebook\n",
    "show(layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72710e11-d240-42b0-8745-e5cda6c9630a",
   "metadata": {},
   "source": [
    "To build a semantic search application, we need something to help automate the process of storing and searching our vectors. We can use a vector database to do this. The next section will show how to use the Milvus Vector Database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d1c45-af3c-4b54-9b25-c7fc070de722",
   "metadata": {},
   "source": [
    "## 3.3 Vector Database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdb221-6932-4883-b284-ac31cd1e1d39",
   "metadata": {},
   "source": [
    "Milvus is a Vector Database that can be used for quick experimentation without any setup required other than installing their Python library. The following cell will create a database and setup a collection that we will use to store our NVCLIP embeddings. We then use the insert function to add all the embeddings that were just displayed on the plot in the previous section. By putting these embeddings into Milvus, it will make it easier to search the vectors. \n",
    "\n",
    "To learn more about how to use Milvus, visit their [quickstart guide](https://milvus.io/docs/quickstart.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b035f-850b-4583-a9f4-5413b1363738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "#create database \n",
    "client = MilvusClient(\"milvus_demo.db\")\n",
    "\n",
    "if not client.has_collection(collection_name=\"demo_collection\"):\n",
    "    #create collection in database. This will associate a vector with the metadata\n",
    "    client.create_collection(\n",
    "        collection_name=\"demo_collection\",\n",
    "        dimension=1024 #NVCLIP output dimension\n",
    "        )\n",
    "    res = client.insert(collection_name=\"demo_collection\", data=image_embedding_data)\n",
    "    print(res)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fa86b-c957-48ef-abc2-62958a922a92",
   "metadata": {},
   "source": [
    "The database now has our embeddings loaded and we can provide a new vector to search for the most similar embeddings in the database. The following cell will take the string assigned to the `text_search` variable and then use Milvus to search for the 5 most common images in our vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941b87e-b234-4415-b5b2-cf413d10d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed text search query \n",
    "text_search = \"A School Bus\"\n",
    "resp = nvclip([text_search]) #generate embedding \n",
    "text_embedding = resp[\"data\"][0][\"embedding\"]\n",
    "\n",
    "#Search vector DB for closest images \n",
    "client = MilvusClient(\"milvus_demo.db\")\n",
    "results = client.search(collection_name=\"demo_collection\", data=[text_embedding_data[0][\"vector\"]], limit=5, output_fields=[\"file_name\"]) #search for 5 most similar vectors \n",
    "#print results \n",
    "for x in results:\n",
    "    print(x)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e2e20-252d-411b-9149-d036007b1b77",
   "metadata": {},
   "source": [
    "We can wrap this logic in a while loop and plot the most common images to make a simple semantic search app. Run the following cell to search the database of images through text prompts. A text box will appear at the bottom of the cell where you can type your search terms. The closest matching images will then be displayed. This will run in a loop forever so you must stop the cell manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dca305-5a87-4e35-9d15-ddc666defa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    client = MilvusClient(\"milvus_demo.db\")\n",
    "    text_prompt = input(\"Enter Search Term:\")\n",
    "    resp = nvclip([text_prompt])\n",
    "    query_vector = resp[\"data\"][0][\"embedding\"]\n",
    "    results = client.search(collection_name=\"demo_collection\", data=[query_vector], limit=9, output_fields=[\"file_name\"])\n",
    "    client.close()\n",
    "    image_paths = [x[\"entity\"][\"file_name\"] for x in results[0]]\n",
    "    \n",
    "    num_images = 9\n",
    "    grid_size = (3,3)\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(15,15))\n",
    "    fig.subplots_adjust()\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = Image.open(image_paths[i])\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bc784-c8da-4b7d-8cbd-3705d6162f47",
   "metadata": {},
   "source": [
    "## 3.4 Interactive Gradio UI for Semantic Search\n",
    "\n",
    "Now we can put all this together to build a cohesive Gradio UI that allows us to easily search any provided folder of images. The code for this application can be found in the same folder as this notebook under \"main.py\". \n",
    "\n",
    "When the script is launched it will automatically generate embeddings for the passed in folder of images and store them in Milvus. The Gradio UI then allows you to actively search the database through text and image prompts.\n",
    "\n",
    "Run the cell below to launch the Gradio UI. It will start the application with the cropped vehicle images generated earlier in this notebook. After running the cell below, you can access the gradio UI at http://localhost:7860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08261e4-a58f-42a3-b465-7f6c543b3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{python_exe} main.py traffic_data/vehicleannotations/cropped_vehicles {api_key}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
