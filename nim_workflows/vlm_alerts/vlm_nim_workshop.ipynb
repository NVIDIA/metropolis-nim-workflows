{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e217daaa",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.  \n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3b329-928c-45ea-9e2e-0e9b861aaede",
   "metadata": {},
   "source": [
    "# VLM NIM Workshop\n",
    "\n",
    "NVIDIA Inference Microservices (NIMs) are a collection of easy to use API driven microservices to interact with AI models. \n",
    "\n",
    "This workshop focuses on the Visual Language Models that are currently available. \n",
    "\n",
    "- <a href=https://build.nvidia.com/nvidia/neva-22b> Neva-22b </a>\n",
    "- <a href=https://build.nvidia.com/microsoft/microsoft-kosmos-2> Kosmos-2 </a>\n",
    "- <a href=https://build.nvidia.com/adept/fuyu-8b> Fuyu-8b </a>\n",
    "- <a href=https://build.nvidia.com/google/google-paligemma> paligemma </a>\n",
    "- <a href=https://build.nvidia.com/microsoft/phi-3-vision-128k-instruct> phi-3-vision-128k-instruct </a>\n",
    "- <a href=https://build.nvidia.com/nvidia/vila> VILA </a>\n",
    "\n",
    "\n",
    "\n",
    "To learn more about NIMs visit <a href=https://build.nvidia.com/explore/discover> ai.nvidia.com </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afed3f4-8757-470a-8a5c-b5654bc4585a",
   "metadata": {},
   "source": [
    "This workshop has four parts:\n",
    "\n",
    "**Part 0.** Setup Environment  \n",
    "**Part 1.** Text Chat  \n",
    "**Part 2.** Image Chat  \n",
    "**Part 3.** Video Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f51777-9f57-4d74-a188-38695bf55cb5",
   "metadata": {},
   "source": [
    "## Part 0: Setup Environment \n",
    "\n",
    "1) This notebook requires the user to bring a video in mp4 format to run the streaming pipeline in Part 3. The notebook will assume there is a file named **test_video.mp4** in the same directory as this notebook. Adjust the cell below to define a path to a test video file. \n",
    "\n",
    "2) Fill in the *api_key* variable with your API key that was generated from the NIM website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4e5a2-8754-4087-a076-940f3d8b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video = \"test_video.mp4\" #FIXME - point to a 1080p mp4 video file. \n",
    "api_key = \"FIXME\" #FIX ME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4107f13-6040-43dc-9cf2-f300889c1c99",
   "metadata": {},
   "source": [
    "The next cell will install all the necessary Python packages for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac298e-093e-4cb9-885a-e271d57d3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8350981-12c8-4e1a-9532-2bf5350c292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, io, subprocess, base64\n",
    "from time import time \n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import requests \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd26950-2e57-4562-af31-3671ccbc4d61",
   "metadata": {},
   "source": [
    "Ensure that no errors occured during the installation and import in the two cells above before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb349f-0af7-45c5-b926-5543dd22a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup VLM NIM Urls \n",
    "neva_api_url = \"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\"\n",
    "kosmos2_api_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2\"\n",
    "fuyu8b_api_url = \"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\"\n",
    "paligemma_api_url = \"https://ai.api.nvidia.com/v1/vlm/google/paligemma\"\n",
    "phi3_api_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75648197-f234-4752-b6f9-5dcc56746857",
   "metadata": {},
   "source": [
    "## Part 1: Text Chat\n",
    "\n",
    "This section is a simple example to call a VLM NIM with a POST request using just text input. The request will be made up of some headers that will include your API Key for authorization and then the payload with the content for the VLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2538d-4667-4443-b40a-082547326fa2",
   "metadata": {},
   "source": [
    "The API key should be presented as a Bearer token and the request body will be JSON format so we need to specify this in the header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfeb47d-4485-4cac-a904-25803edb7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  \"Authorization\": f\"Bearer {api_key}\",\n",
    "  \"Accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94856f6b-a3b8-43f0-9232-f56e618903c8",
   "metadata": {},
   "source": [
    "The payload is JSON format and follows an API schema similar to the OpenAI API. For full details on the API Spec for NIMs, visit these pages:\n",
    "- https://docs.api.nvidia.com/nim/reference/nvidia-neva-22b  \n",
    "- https://docs.api.nvidia.com/nim/reference/microsoft-kosmos-2\n",
    "- https://docs.api.nvidia.com/nim/reference/adept-fuyu-8b\n",
    "- https://docs.api.nvidia.com/nim/reference/google-paligemma\n",
    "- https://docs.api.nvidia.com/nim/reference/microsoft-phi-3-vision-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695de32-d15b-4166-8a95-05a7d53472ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f'Can you tell me what you are capable of?'\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 1024,\n",
    "  \"temperature\": 0.20,\n",
    "  \"top_p\": 0.70,\n",
    "  \"stream\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95005b26-9834-4930-90b2-615da556b194",
   "metadata": {},
   "source": [
    "The payload is made up of two main parts, the messages and the hyperparameters. \n",
    "\n",
    "The messages key and associated list value is the set of all messages between the \"user\" and the \"assistant\". The \"user\" being the person interacting with the model and the \"assistant\" being the VLM. \n",
    "\n",
    "In this notebook we will only send single messages with the \"user\" role, but to implement a full chatting experience, the return of the VLM would be appended to these messages as the \"assistant\" then followed up by the next \"user\" message. This creates a multi turn chat that has the history of the conversation for the VLM to use. \n",
    "\n",
    "In the payload below the \"messages\" field are a set of hyperparameters that can be controlled to tune the VLM. \n",
    "\n",
    "- max_tokens: Maximum number of tokens to generate for the response. \n",
    "- temperature: The randomness of the output. Higher temperature allows for less likely values to be chosen in the output.   \n",
    "- top_p: Also controls the randomness of the output. Higher top_p will make the LLM choose more likely values. \n",
    "- stream: Streaming responses can be used to get tokens as soon as they are generated instead of waiting for the complete response. \n",
    "\n",
    "For simplicity, this notebook will not use streaming responses. To see how to do this, visit the documentation pages linked in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80ba53-5184-40b0-b986-7c40df92cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(neva_api_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "\n",
    "#print the full JSON response \n",
    "json_string = json.dumps(response, indent=4, sort_keys=True)\n",
    "print(json_string)\n",
    "\n",
    "#print only the reply \n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9b8af-9fa6-4ae3-baa9-743e3df8a3b9",
   "metadata": {},
   "source": [
    "With the headers and payload setup, the Python requests library can be used to send a POST request to the VLM API url. The response will be in JSON format and can be parsed and then accessed to get the reply from the VLM. \n",
    "\n",
    "To summarize, Part 1 covered how to call and interact with VLM NIMs using just text input. The API schema and style of requests shown in this part is the same if you were to interact with the LLM NIMs as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61d2b9-f38a-4a42-93dc-46c4ed88840f",
   "metadata": {},
   "source": [
    "## Part 2: Image Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee64f9-cd94-414e-8792-a065e7c07e41",
   "metadata": {},
   "source": [
    "The VLMs are unique because unlike LLMs, they can accept visual and text input. This section will cover the basics on how to provide Images to the VLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fedbe-f901-4a66-bc66-105da1fc89ad",
   "metadata": {},
   "source": [
    "### Part 2.1: Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e4282-87be-45c9-b92f-c213e724d84f",
   "metadata": {},
   "source": [
    "Images are often provded to the multimodal models at a lower resolutions such as 224x224 or 336x336. This is based on the input size of the vision encoder used in the VLM. \n",
    "\n",
    "To reduce our request size, we can preprocess the image to the input resolution used by the VLM. This is not strictly necessary as the NIM itself will process the image to the correct input size but we can reduce latency and API calls by converting our image to JPEG and downsizing it before uploading it through the request. \n",
    "\n",
    "After the image processing is done, it is converted to a base 64 string. A base 64 string encoded image is a common way to serialize images when they are included directly with a REST API request. \n",
    "\n",
    "The maximum image size supported when included directly in the reponse in 180,000 bytes. For larger files, the <a href=https://docs.api.nvidia.com/cloud-functions/reference/createasset> large asset API </a> can be used. This requires a few more API calls but allows for large files to be given to the NIMs.  \n",
    "\n",
    "For this notebook, our image sizes can be reduced sufficiently to allow for upload directly in the chat completion requests so the large asset API is not needed. \n",
    "\n",
    "We will use this image for our testing: \n",
    "\n",
    "![Test Image](test_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411e4df-80a9-4cde-b7ee-3ae41a698924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \"\"\" Resize image, encode as jpeg to shrink size then convert to b64 for upload \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "    elif isinstance(image, Image.Image):\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    image = image.resize((336,336)) #Resize or center crop and padding to be square are common approaches \n",
    "    buf = io.BytesIO() #temporary buffer to save processed image \n",
    "    image.save(buf, format=\"JPEG\") #save as jpeg to reduce size\n",
    "    image = buf.getvalue()\n",
    "    image_b64 = base64.b64encode(image).decode() #convert to b64 string\n",
    "    assert len(image_b64) < 180_000, \"Image to large to upload.\" #ensure image is small enough\n",
    "    return image_b64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e9e6b-278d-4cbe-a91b-74a4fc96b58b",
   "metadata": {},
   "source": [
    "After processing the image, this is what the VLM will see:\n",
    "\n",
    "![Test Image](test_image_resized.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4edf2d-3481-41ff-b981-d4b44d9dbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  \"Authorization\": f\"Bearer {api_key}\",\n",
    "  \"Accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc6d31-5939-4cbc-8032-73a9094e70f5",
   "metadata": {},
   "source": [
    "The headers are configured the same way as in Part 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546265a6-ce77-4dcc-a659-20831ceeb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_b64 = process_image(\"test_image.png\") #get the base 64 representation of our reduced size image \n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f'Describe what you see in this image. <img src=\"data:image/jpeg;base64,{image_b64}\" />'\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 1024,\n",
    "  \"temperature\": 0.20,\n",
    "  \"top_p\": 0.70,\n",
    "  \"seed\": 0,\n",
    "  \"stream\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b5dc2-529e-418e-b141-99fcfddbce3f",
   "metadata": {},
   "source": [
    "In the payload, our content field now contains our image by supplying an image tag with our prompt and referencing the base 64 string.\n",
    "\n",
    "```<img src=\"data:image/jpeg;base64,{image_b64}\"/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f9a7a-9289-4773-a605-edd5f2435343",
   "metadata": {},
   "source": [
    "Now when the POST request is sent, the VLM will take into account the image in our prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428d186-1fed-4b25-9e0f-4c440c7fb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(neva_api_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "\n",
    "#print only the reply \n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df0d1f",
   "metadata": {},
   "source": [
    "### Part 2.1.2 Exercise - Try on your own image with your own queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2e703",
   "metadata": {},
   "source": [
    "In this part, try these APIs on your own images. Play with the 4 hyperparameters and see what type of responses it generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12042c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_b64 = process_image(\"#FIXME\") #put the filepath to your own image\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f'#FIXME <img src=\"data:image/jpeg;base64,{image_b64}\" />'\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 1024, #TEST\n",
    "  \"temperature\": 0.20, #TEST\n",
    "  \"top_p\": 0.70, #TEST\n",
    "  \"seed\": 0, #TEST\n",
    "  \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(neva_api_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "\n",
    "#print only the reply \n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2761b-eb2f-450f-af75-2cb3dfe6290d",
   "metadata": {},
   "source": [
    "### Part 2.2 Abstracting the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef56626-5314-4ba6-830f-a759becd19ee",
   "metadata": {},
   "source": [
    "For easier use, we can wrap the API requests and image processing into a simple callable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f6afb-91b9-4a5d-b72f-1f54abf2a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLM:\n",
    "    def __init__(self, url, api_key):\n",
    "        \"\"\" Provide NIM API URL and an API key\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.url = url \n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Accept\": \"application/json\"}\n",
    "\n",
    "    def _encode_image(self, image):\n",
    "        \"\"\" Resize image, encode as jpeg to shrink size then convert to b64 for upload \"\"\"\n",
    "\n",
    "        if isinstance(image, str): #file path\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        elif isinstance(image, Image.Image): #pil image \n",
    "            image = image.convert(\"RGB\")\n",
    "        elif isinstance(image, np.ndarray): #cv2 / np array image \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image)\n",
    "        else:\n",
    "            print(f\"Unsupported image input: {type(image)}\")\n",
    "            return None \n",
    "            \n",
    "        image = image.resize((336,336))\n",
    "        buf = io.BytesIO()\n",
    "        image.save(buf, format=\"JPEG\")\n",
    "        image = buf.getvalue()\n",
    "        image_b64 = base64.b64encode(image).decode()\n",
    "        assert len(image_b64) < 180_000, \"Image too large to upload.\"\n",
    "        return image_b64\n",
    "\n",
    "    def __call__(self, prompt, image):\n",
    "        \"\"\" Call VLM object with the prompt and path to image \"\"\" \n",
    "        image_b64 = self._encode_image(image)\n",
    "\n",
    "        #For simplicity, the image will be appended to the end of the prompt. \n",
    "        payload = {\n",
    "              \"messages\": [\n",
    "                {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": f'{prompt} Here is the image: <img src=\"data:image/jpeg;base64,{image_b64}\" />'\n",
    "                }\n",
    "              ],\n",
    "              \"max_tokens\": 128,\n",
    "              \"temperature\": 0.20,\n",
    "              \"top_p\": 0.70,\n",
    "              \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.url, headers=headers, json=payload)\n",
    "        response = response.json()\n",
    "        reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return reply, response #return reply and the full response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d7be-7dc2-4862-898c-ac4cad6b3394",
   "metadata": {},
   "source": [
    "Now we have an easy to use VLM class that wraps a multimodal NIM. \n",
    "\n",
    "Lets run some images against Neva, Kosmos and Fuyu to compare. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f9ec3-8644-4e93-b8c4-7b23cd50193e",
   "metadata": {},
   "source": [
    "### Part 2.3.1 Comparing VLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f1455-87ab-4d78-80fc-7abddbce102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a VLM object for each supported model \n",
    "phi3 = VLM(phi3_api_url, api_key)\n",
    "neva = VLM(neva_api_url, api_key)\n",
    "fuyu8b = VLM(fuyu8b_api_url, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae75e1b-6a12-4a4b-ab37-d92af04fbd69",
   "metadata": {},
   "source": [
    "Adjust the *custom_prompt* variable if you want to try different prompts.   \n",
    "Adjust the *image_path* variable if you want to try different images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271ed66-36a3-4a10-8625-8acb8cd3719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"Can you tell me about the image?\" #CHANGE ME\n",
    "image_path = \"test_image.png\" #CHANGE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4d5a3-a992-4a42-b544-4f4d66e69c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEVA\n",
    "start_time = time()\n",
    "response, _ = neva(custom_prompt, image_path)\n",
    "print(f\"Neva Response: {response}\")\n",
    "print(f\"Neva Time: {time() - start_time} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a102a-df48-4315-9dc4-ce80d1cd1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUYU\n",
    "start_time = time()\n",
    "response, _ = fuyu8b(custom_prompt, image_path)\n",
    "print(f\"Fuyu-8b Response: {response}\")\n",
    "print(f\"Fuyu-8b Time: {time() - start_time} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd26602-d96f-4ab0-b005-ecb1fd65154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phi3\n",
    "start_time = time() \n",
    "response, _= phi3(custom_prompt, image_path)\n",
    "print(f\"Phi-3 Response: {response}\")\n",
    "print(f\"Phi-3 Time: {time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96a31f-4e58-49a0-84fc-200ba82a6d8b",
   "metadata": {},
   "source": [
    "Now we can see the time each model takes to send a request as well as the difference in their outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97874ca-e67b-42b1-997c-52df4e960821",
   "metadata": {},
   "source": [
    "### Part 2.3.2 Exercise - Try different VLM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b0642-6b44-4e4d-ade9-f7da8cb1010f",
   "metadata": {},
   "source": [
    "The following VLM API variables were defined in the beginning of this notebook:\n",
    "\n",
    "- neva_api_url \n",
    "- kosmos2_api_url\n",
    "- fuyu8b_api_url\n",
    "- paligemma_api_url \n",
    "- phi3_api_url\n",
    "\n",
    "In the cell below, change the FIX_ME to one of the model api url variables to try out different VLMs with the VLM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0177e-225a-413e-aa55-4f4c5a77891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vlm = VLM(FIX_ME, api_key) #Change the FIX_ME to one of the x_api_url variables to test different models. \n",
    "custom_prompt = \"Can you tell me about the image?\" #CHANGE ME\n",
    "image_path = \"test_image.png\" #CHANGE ME\n",
    "\n",
    "start_time = time() \n",
    "vlm_response, full_response = custom_vlm(custom_prompt, image_path)\n",
    "print(f\"VLM Response: {vlm_response}\")\n",
    "print(f\"VLM Time: {time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff250475-f0b5-42e4-8325-f39f79da1feb",
   "metadata": {},
   "source": [
    "### Part 2.4 Grounding with Kosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179d186-a296-441d-9af1-109a0458909c",
   "metadata": {},
   "source": [
    "The Kosmos model is designed with grounding capability. Allowing it to localize areas in the image. This is particularly usefull for questions that require more precise answers such as counting and positioning. \n",
    "\n",
    "In the full response from Kosmos, we get bounding boxes of objects detected in addition to the text reply. This is a unique feature of the Kosmos model. The bounding box overlay can be viewed on the NIM demo for Kosmos https://build.nvidia.com/microsoft/microsoft-kosmos-2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab9aa9",
   "metadata": {},
   "source": [
    "#### Part 2.4.1 List the Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ca147-d0c9-4627-b4aa-7e70ff848aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kosmos2 = VLM(kosmos2_api_url, api_key)\n",
    "kosmos2_response, full_response = kosmos2(\"Can you show me where all the cars are?\", \"test_image.png\")\n",
    "print(json.dumps(full_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1a51c-c9c9-41b1-ae62-39ec4fe064a2",
   "metadata": {},
   "source": [
    "The bounding boxes are associated with specific substrings in the output. Allowing parts of the output such as \"cars\" to then be grounded by a set of bounding boxes that indicate where cars are present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c814dfb",
   "metadata": {},
   "source": [
    "#### Part 2.4.2 Visualize the Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94b2af",
   "metadata": {},
   "source": [
    "In the above response, the kosmos model predicted 2 distinct entities - \"all the cars\" and \"the highway\". Let's draw bounding boxes around each of the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4794fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('test_image.png')\n",
    "width, height = im.size\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(im)\n",
    "\n",
    "entities = full_response[\"choices\"][0][\"message\"][\"entities\"]\n",
    "\n",
    "bbox_colors = ['b', 'r', 'g']\n",
    "count=0\n",
    "\n",
    "for objects in entities:\n",
    "    for bbox in objects[\"bboxes\"]:\n",
    "        \n",
    "        x0_scaled = bbox[0]*width\n",
    "        y0_scaled = bbox[1]*height\n",
    "        x_len = (bbox[2]*width)-x0_scaled\n",
    "        y_len = (bbox[3]*height)-y0_scaled\n",
    "        \n",
    "        rect = patches.Rectangle((x0_scaled, y0_scaled), x_len, y_len, linewidth=1, edgecolor=bbox_colors[count], facecolor='none')\n",
    "        plt.text(x0_scaled, y0_scaled, objects[\"phrase\"])\n",
    "        \n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "    count+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49812729-a166-4971-add9-c633b2d0a1db",
   "metadata": {},
   "source": [
    "# Part 3: Video Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3965cb9-181a-4247-aab8-1ff72ef875d2",
   "metadata": {},
   "source": [
    "With the basics covered, we can now build a more useful pipeline that will give us insights into live streaming videos.\n",
    "\n",
    "Streaming videos from security cameras, drones, etc. generates a lot of useful video. However, it is a challenge to extract insights from the videos due to the sheer amount of footage that gets recorded. \n",
    "\n",
    "A common scenario is the need to be alerted when certain events are caught on camera such a fire, smoke or a person appearing in a restricted area. \n",
    "\n",
    "Instead of manually monitoring a live stream 24/7, we can use a VLM NIM to monitor a live stream for us. We can then tell the VLM what to look for such as a fire or smoke and the VLM can respond when it detects it on the live stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922183eb-6528-4870-98db-dd7537fd3e98",
   "metadata": {},
   "source": [
    "### Part 3.1: Simple Video Pipeline \n",
    "\n",
    "We can start by building a pipeline that can open a video file and call the VLM. The sample video we will use has 2 scenes. The first scene is ariel footage of mountain landscape and then it transitions to a scene with fire and smoke. \n",
    "\n",
    "We can setup the prompt to be \"Is there a fire in the image? Answer yes or no\". The VLM will follow our instructions and tell us yes or no when it detects a fire in the video. This could then be parsed by another script to take some actions such as sending a notification to the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f559d-3e4e-4a51-8bfd-fa04c60155de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load video and run vlm in a loop with prompt \n",
    "vlm = VLM(neva_api_url, api_key)\n",
    "video_path = test_video \n",
    "prompt  = \"Is there a fire in the image? Answer yes or no.\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) #open video file with openCV\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        continue \n",
    "\n",
    "    reply = vlm(prompt, frame)\n",
    "    print(reply)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bdf015-799f-42e9-b2b1-d79cf7cb7be9",
   "metadata": {},
   "source": [
    "The simple pipeline is still quite limited as it will only work on a video file and process it frame by frame. \n",
    "\n",
    "Often, it is not necessary to process every frame as some events will change slowly over time. \n",
    "\n",
    "We can adjust the pipeline to make it more friendly for video streaming use cases. \n",
    "\n",
    "Additionally we want to see the output visually instead of through the command line. We can create an overlay window and print the VLM response. \n",
    "\n",
    "We also want to take advantage of the VLM's natural language ability. The strength of the VLM is its ability to change and be interactive. To enable this, we can setup a REST API server to dynamically change the prompt to the VLM as it is processing the video stream. \n",
    "\n",
    "These features have been implemented in the main.py, api_server.py and vlm.py scripts that are in the same directory as this notebook. This more advanced pipeline will be run in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d291d8-a00d-459e-bda5-c748e0f3fb9c",
   "metadata": {},
   "source": [
    "### Part 3.2 Interactive Video Pipeline\n",
    "\n",
    "We can launch this more advanced pipeline in the background with a python subprocess. Then interact with it through REST requests in this notebook to change the prompt. When launched, an overlay window will appear which will show the VLM responses on top of the input video stream. \n",
    "\n",
    "The video streaming pipeline is in the main.py script. The VLM class defined above is in vlm.py with some extra logic to handle threading and dynamic prompts. The API server is in api_server.py. Explore these scripts to see how this more complex pipeline is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f09b5-e1c6-4ea3-997e-c8d2ce3b6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_available = {\"neva\":\"https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b\", \"kosmos-2\":\"https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2\", \"fuyu-8b\":\"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\", \"paligemma\":\"https://ai.api.nvidia.com/v1/vlm/google/paligemma\", \"phi-3-vision\":\"https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\"}\n",
    "launched_processes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2acfd6-6f89-4758-9ef5-a11cac178b02",
   "metadata": {},
   "source": [
    "Configure the cell below to adjust the model and video file used to run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c1a67-b5b3-4240-ae44-ecd7c1e41d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = \"neva\"  #[\"neva\", \"kosmos-2\", \"fuyu-8b\", \"paligemma\", \"phi-3-vision\"]\n",
    "video_file = test_video #To change the video file, adjust this variable to an mp4 file path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab132e5-c524-4ce3-8a35-eb1f7427523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 5432\n",
    "model_url = models_available[model_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0253f-0689-4c38-9348-314bd19a3d23",
   "metadata": {},
   "source": [
    "Launch Video Pipeline in subprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9987d7-e769-41f6-9140-e58752eab844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "python_exe = sys.executable\n",
    "print(python_exe)\n",
    "\n",
    "log = open(\"log_file.txt\", \"w+\")\n",
    "process = subprocess.Popen([python_exe, \"main.py\", \"--model_url\", model_url, \"--video_file\", video_file, \"--api_key\", api_key, \"--port\", str(port), \"--overlay\", \"--loop_video\"], stdout=log, stderr=log)\n",
    "print(process)\n",
    "launched_processes.append(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce52c9-d342-42c6-af4d-79e89256b6c5",
   "metadata": {},
   "source": [
    "Once the subprocess is launched, a window should pop up that is playing the video with a text overlay showing the output of the VLM. \n",
    "\n",
    "Run the cells below to send API requests to change the prompt input to the VLM. You should see the output of the VLM change a few seconds after sending the new prompt. \n",
    "\n",
    "You can send a one time query or a continous alert. You can run the cells below several times and adjust the *prompt* variable to see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d7232-41b7-41d6-a496-b7e083b883c9",
   "metadata": {},
   "source": [
    "Send a query that will be evaluated only one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad683e-9873-4d25-a5ff-05d502726d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send query \n",
    "prompt = \"Can you describe the scene?\" #CHANGE ME\n",
    "\n",
    "params = {\"query\":prompt, \"alert\":False}\n",
    "url = f\"http://localhost:{port}/query\"\n",
    "response = requests.get(url, params=params)\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d23b6b-9142-4d6f-8f23-18e92b239946",
   "metadata": {},
   "source": [
    "Send an an alert that will be continuously evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfaeaf-ad75-4cf1-820b-17cec51cab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send alert. This will be evaluated continuously.  \n",
    "prompt = \"Is there a fire? Answer yes or no.\" #CHANGE ME\n",
    "\n",
    "params = {\"query\":prompt, \"alert\":True} #set alert to True to enable continuous evaluation\n",
    "url = f\"http://localhost:{port}/query\"\n",
    "response = requests.get(url, params=params)\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803e50f-ac66-40a5-aea5-fe75475f1730",
   "metadata": {},
   "source": [
    "* Note: If the the REST API request hangs for a long time or is not able to connect, modify the *url* variable and replace the localhost with the IP address of your computer. This can be found through the command line with the **ipconfig** or **ifconfig** commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bded66-06e3-41df-b81f-abbe658b58d4",
   "metadata": {},
   "source": [
    "Run the cell below to kill the subprocess that is running the streaming pipeline. You can also click on the overlay window and press the 'q' key to quit the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42cbe8-7427-4ca0-b188-0e2d75db04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up sub process\n",
    "for p in launched_processes:\n",
    "    p.terminate() \n",
    "log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
