{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f46294-b685-40a0-a61f-2cb2d9bc0b17",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.  \n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3173c72-3583-4418-bbcc-57073bddead0",
   "metadata": {},
   "source": [
    "# NVDINOv2 NIM Workshop\n",
    "\n",
    "NVIDIA Inference Microservices (NIMs) are a collection of easy to use API driven microservices to interact with AI models. \n",
    "\n",
    "This workshop focuses on using NVDINOv2 which is a general purpose image embedding model. NVDINOv2 can be combined with any classifcation head or a KNN algorithm and vector database to create a few shot classification model. The benefit of this approach is it does not require extensive model training or signficiant compute resources because NVDINOv2 has already been trained on vast amounts of data which allows it to produce feature rich embeddings that can be used for downstream tasks.   \n",
    "\n",
    "To learn more about NIMs visit <a href=https://build.nvidia.com/explore/discover> ai.nvidia.com </a>  \n",
    "\n",
    "![Few Shot Arch Diagram](readme_assets/few_shot_arch_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22f537-399b-46cb-b65a-5c796b674eb6",
   "metadata": {},
   "source": [
    "This workshop has four parts:\n",
    "\n",
    "**Part 0.** Setup Environment  \n",
    "**Part 1.** NVDINOv2 Requests   \n",
    "**Part 2.** Prepare Dataset  \n",
    "**Part 3.** Few Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e258c-ec8b-4829-a24e-da4334e173ec",
   "metadata": {},
   "source": [
    "# Part 0: Setup Environment\n",
    "\n",
    "***In the following cell, paste your NIM API key*** \"nvapi-***\" to set the ```api_key``` variable. Then continue running the cells to install the dependecies. Note that this notebook will automatically download a public car classification dataset that is 6GB and will be used throughout the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d33244-a8de-4876-8b7f-0ccae542fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"nvapi-***\" #FIX ME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d8645-4950-41d2-a510-005470ddb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependecies\n",
    "import sys \n",
    "python_exe = sys.executable\n",
    "!{python_exe} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff8111-477a-446a-8b0f-b46701f3446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pymilvus import MilvusClient\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import requests \n",
    "import io \n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, CustomJS\n",
    "from bokeh.layouts import column\n",
    "from bokeh.io import output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np \n",
    "from bokeh.palettes import Dark2_5 as palette\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f956-4325-40d9-ac89-ecef638cec35",
   "metadata": {},
   "source": [
    "Ensure that no errors occured during the installation and import in the two cells above before continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c487943-aebf-4ad6-8679-17065f9fe2b6",
   "metadata": {},
   "source": [
    "# Part 1: NVDINOv2 Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d95eb0-9e5e-46d4-b1bf-7e12fd2e3ef3",
   "metadata": {},
   "source": [
    "To generate an image embedding with NVDINOv2, the first step is to upload the image through the NVCF large asset API, then make the NVDINOv2 NIM API call. \n",
    "\n",
    "NVDINOv2 has an input resolution of 960x544. Most images of this size are too large to include directly in the NVDINOv2 NIM API call so the large asset API is used first to upload the image. Once the image is uploaded, a unique ID is returned that can be used to reference the image in the NVDINOv2 request.\n",
    "\n",
    "Sending a request to the NVDINOv2 NIM API requires a header that includes your API key for authorization and a payload with the content to be embedded. \n",
    "\n",
    "In the header, the API key should be presented as a Bearer token and the request body is JSON format. \n",
    "\n",
    "For further Reference:\n",
    "* [NVDINOv2 NIM API documentation](https://docs.api.nvidia.com/nim/reference/nvidia-nv-dinov2-infer)\n",
    "* [NVCF large asset API documentation](https://docs.api.nvidia.com/cloud-functions/reference/createasset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429f87d-7b57-4967-ac7f-d9702639e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URLs and key \n",
    "assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\" #large asset upload\n",
    "nvdinov2_url = \"https://ai.api.nvidia.com/v1/stg/cv/nvidia/nv-dinov2\" #nvdinov2 endpoint \n",
    "header_auth = f\"Bearer {api_key}\" #authentication to include in headers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec646d3a-ddbf-4479-8489-346c03c6972c",
   "metadata": {},
   "source": [
    "The first step is to create an asset ID and get an upload link for the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e4410-7566-4c95-9b57-d7443c747e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1) Send request to get an upload link \n",
    "assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
    "headers = {\n",
    "    \"Authorization\": header_auth,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"accept\": \"application/json\",\n",
    "}\n",
    "payload = {\"contentType\": f\"image/jpeg\", \"description\": \"input image\"}\n",
    "response = requests.post(assets_url, headers=headers, json=payload, timeout=30)\n",
    "response.raise_for_status()\n",
    "\n",
    "#asset_url is the upload link and asset_id is a unique identifier to reference the image\n",
    "asset_url = response.json()[\"uploadUrl\"]\n",
    "asset_id = response.json()[\"assetId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590baaee-c219-4c1b-b286-47e05b8c22b8",
   "metadata": {},
   "source": [
    "We now have an asset ID and an AWS S3 link to upload the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020537c7-dfbf-43a9-9296-b8ef50130cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2) Upload image to asset_url \n",
    "s3_headers = {\n",
    "    \"x-amz-meta-nvcf-asset-description\": \"input image\",\n",
    "    \"content-type\": f\"image/jpeg\",\n",
    "}\n",
    "# Convert image to jpeg before uploading\n",
    "image = Image.open(\"readme_assets/few_shot_arch_diagram.png\").convert(\"RGB\")\n",
    "buf = io.BytesIO()  # temporary buffer to save image\n",
    "image.save(buf, format=\"JPEG\") #convert image to jpeg to get smaller upload size \n",
    "\n",
    "# upload image\n",
    "response = requests.put(\n",
    "    asset_url,\n",
    "    data=buf.getvalue(),\n",
    "    headers=s3_headers,\n",
    "    timeout=300,\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e29011-7160-4aaa-be5d-851fe421238c",
   "metadata": {},
   "source": [
    "The image has been uploaded and it can now be referenced using the asset ID in any NIM API requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd55a8-eb50-4e3b-952f-c18744e6aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3) Send NVDINOv2 Request and reference uploaded image\n",
    "payload = {\"messages\": []} #payload can be just an empty \"messages\" field. Since the image is referenced in the header, no other informnation is needed in the payload. \n",
    "asset_list = f\"{asset_id}\"\n",
    "\n",
    "#Asset ID needs to be included in the header\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"NVCF-INPUT-ASSET-REFERENCES\": asset_id,\n",
    "    \"NVCF-FUNCTION-ASSET-IDS\": asset_id,\n",
    "    \"Authorization\": header_auth,\n",
    "}\n",
    "\n",
    "#Send NVDINOv2 request to generate the embedding\n",
    "response = requests.post(nvdinov2_url, headers=headers, json=payload)\n",
    "response = response.json()\n",
    "embedding = response[\"metadata\"][0][\"embedding\"] #get the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987d922-fa11-47d6-b682-69c70dfe9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding))\n",
    "print(type(embedding[0]))\n",
    "#print(embedding) #uncomment to print entire embedding vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd465ba-5ec7-4e6f-9183-1c0f26b64d2a",
   "metadata": {},
   "source": [
    "In the response, we can get the embedding of our image. This is a 1536d vector that represents our image and can be used for downstream tasks such as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9ed42-0933-4df4-8ce2-f081f165aa68",
   "metadata": {},
   "source": [
    "## Part 2: Prepare Dataset\n",
    "\n",
    "To show how to use NVDINOv2 embeddings for few shot classification, we can use a [car classification dataset from HuggingFace](https://huggingface.co/datasets/tanganke/stanford_cars). The following cell will download the 6GB dataset. For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d885f66-b5bd-412e-8a44-5fabe4d54aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tanganke/stanford_cars\") #6GB\n",
    "train_set = dataset[\"train\"]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe626-1fb5-4ddf-a86f-3ae4c3513b70",
   "metadata": {},
   "source": [
    "It takes 1 NIM credit to embed 1 image so we will generate a much smaller subset of the data to show how to build a few shot classification model. The subset will have three classes. Each class will have 10 test images and 5 train images. You can adjust the cell below to control the data in the subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3a53d-f4e2-40c6-aa8d-6e7b53696460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 classes with 5 train images and 10 test images. \n",
    "num_classes = 3\n",
    "test_images_per_class = 10\n",
    "train_images_per_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdbc7-63ff-44ff-a61f-b4156140fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(dataset, classes, images_per_class):\n",
    "    \"\"\"Make subset with given list of classes and specified images per class\"\"\"\n",
    "    subset = []\n",
    "    label_counter = {}\n",
    "    for i, sample in enumerate(dataset):\n",
    "        label = sample[\"label\"]\n",
    "        if label not in classes:\n",
    "            continue \n",
    "            \n",
    "        label_count = label_counter.get(label, 0)\n",
    "    \n",
    "        if label_count < images_per_class:\n",
    "            subset.append(sample)\n",
    "            label_counter[label] = label_counter.get(label, 0) + 1\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67464-87d7-4457-863f-c9c6a68668ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make subsets to train and test on\n",
    "train_subset = make_subset(dataset[\"train\"], range(num_classes), train_images_per_class)\n",
    "test_subset = make_subset(dataset[\"train\"], range(num_classes), test_images_per_class)\n",
    "print(len(train_subset))\n",
    "print(len(test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499473c-5477-494e-9812-6fed0cdb4fdc",
   "metadata": {},
   "source": [
    "To make it easier to generate the embeddings, a NVDINOv2 wrapper classes has been implemented in the nvdinov2.py script in the same directory as this notebook. This will handle the image upload and embedding calls. It can be passed a list of image paths or PIL images. It will return a list of embeddings for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f3b6d-b03f-4c6d-87f1-0ace87ca509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvdinov2 import NVDINOv2\n",
    "def add_embeddings(api_key, dataset):\n",
    "    nvdinov2 = NVDINOv2(api_key)\n",
    "    pil_images = [x[\"image\"] for x in dataset] #get PIL images from dataset\n",
    "    embeddings = nvdinov2(pil_images) #pass list of images to nvdinov2\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i][\"embedding\"] = embeddings[i]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96a309-f4e0-457b-85e5-00aef2436642",
   "metadata": {},
   "source": [
    "NVCLIP is another embedding model availalbe as a NIM that can also be used for few shot classification. If you want to see how it compares to NVDINOv2, then uncomment the cell below to replace the embeddings from NVDINOv2 with embeddings from NVCLIP and run the rest of the notebook. If you want to use NVCLIP, then you will also need change the embedding dimension in section 3.2 from 1536 to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecf3a9-9218-4686-ad84-ff7373d01f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nvclip import NVCLIP\n",
    "# def add_embeddings(api_key, dataset):\n",
    "#     nvclip = NVCLIP(api_key)\n",
    "#     pil_images = [x[\"image\"] for x in dataset]\n",
    "#     print(len(pil_images))\n",
    "#     embeddings = nvclip(pil_images)\n",
    "#     print(embeddings)\n",
    "#     print(len(embeddings))\n",
    "#     for i in range(len(dataset)):\n",
    "#         dataset[i][\"embedding\"] = embeddings[i]\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f1972-253d-431f-ac43-4c7b0c2ad13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the embeddings to the test and train dataset \n",
    "train_subset = add_embeddings(api_key, train_subset)\n",
    "test_subset = add_embeddings(api_key, test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1686e61-278d-42b4-8781-138135caf513",
   "metadata": {},
   "source": [
    "The following cell will plot the test data in 2 dimensions so it can be visually inspected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9697aab-ac16-434b-9fa2-970fe842ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Bokeh output in Jupyter Notebooks\n",
    "output_notebook()\n",
    "vectors = np.array([x[\"embedding\"] for x in test_subset])\n",
    "class_labels =  np.array([x[\"label\"] for x in test_subset])\n",
    "\n",
    "\n",
    "#Use TSNE to project the embeddings to 2D\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=5,\n",
    "    learning_rate=200,\n",
    "    early_exaggeration=5,\n",
    "    n_iter=2000,\n",
    "    random_state=42,\n",
    "    metric=\"cosine\",\n",
    ")\n",
    "embedding_2d = tsne.fit_transform(vectors)\n",
    "\n",
    "p = figure(\n",
    "    title=\"Embedding Visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n",
    ")\n",
    "colors = itertools.cycle(palette)\n",
    "\n",
    "#Plot each class \n",
    "for n in np.unique(class_labels):\n",
    "    indices = np.where(class_labels == n)\n",
    "    n_vectors = embedding_2d[indices]\n",
    "    x = n_vectors[:, 0]\n",
    "    y = n_vectors[:, 1]\n",
    "\n",
    "    source = ColumnDataSource(dict(x=x, y=y))\n",
    "\n",
    "    p.scatter(\"x\", \"y\", source=source, size=8, color=next(colors), legend_label=str(n))\n",
    "\n",
    "# Layout\n",
    "layout = column(p)\n",
    "# Show plot in notebook\n",
    "show(layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab9ba6-fd2e-4bc3-acea-57d203c8f626",
   "metadata": {},
   "source": [
    "Now that the image embeddings have been generated, few shot classifcation can be implemented using models from SKLearn or with a KNN algorithm and a Milvus vector database. Both methods will be explored in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd8329-5816-4182-bc06-798bfa64e98c",
   "metadata": {},
   "source": [
    "# Part 3: Few Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f85a0-2d9d-4bb7-aa80-0d624f935792",
   "metadata": {},
   "source": [
    "At this stage, each image in our test and train datasets has an associated image embedding generated by NVDINOv2 or NVCLIP. These image embeddings are compressed versions of the image that contain the most important information needed to understand the contents of the image. A property of these embeddings, is that images that are similar to each other will be close together in the embedding space. In the plot from Part 2, images of the same class should appear near each other and form clusters. Because these embeddings (also known as feature vectors) contain enough information to differentiate images of different classes, they can be used as input to simple classification models such as Logisitic Regression or a KNN algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eece0-3712-48c2-9e95-4c08e2fb2b28",
   "metadata": {},
   "source": [
    "## Part 3.1 SKLearn \n",
    "\n",
    "SKLearn provides several classification models that can be used with the image embeddings. Each classification model requires a set of features and labels to train. In this case the embeddings are the features and the class ID is the label. \n",
    "\n",
    "Now we can combine the embeddings with a light weight classification head from SKLearn such as [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [K Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). A benefit of using a powerful embedding model is it lets us achieve high accuracy with very few images. This drastically reduces the amount of computation needed to produce the classification model and can be done without a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628144e-591a-4d41-81d1-ec29494b483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Split into labels and features \n",
    "x_test = [x[\"embedding\"] for x in test_subset]\n",
    "y_test = [x[\"label\"] for x in test_subset]\n",
    "\n",
    "x_train = [x[\"embedding\"] for x in train_subset]\n",
    "y_train = [x[\"label\"] for x in train_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea7f70-2141-40b2-b905-b5280a58314a",
   "metadata": {},
   "source": [
    "The following two cells with train and test a logistic regression and KNN classifaction models on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70db7c-5e6c-4b3f-a293-fd0906e8d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and test logistic regression head \n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61515069-07a6-424b-b013-d78b6f82f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and test knn classification head \n",
    "model = KNeighborsClassifier(n_neighbors=3, weights=\"distance\")\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78f52d-b041-4f60-a2bf-fe753471e4d0",
   "metadata": {},
   "source": [
    "From the classification report, you can see that the models can get 90% accuracy with only 5 training images per class and it required very little compute resources to generate the few shot classifiation models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4a769-c52a-4058-be0d-e865c458b41d",
   "metadata": {},
   "source": [
    "## Part 3.2 Milvus Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a236920-af17-42bc-99b0-056948d51ddb",
   "metadata": {},
   "source": [
    "One advantage of using a K-Nearest Neighbors (KNN) classification algorithm is that it can be efficiently scaled and implemented with a vector database. A vector database enables the quick insertion of new image embeddings and allows for fast similarity searches. We've already discussed that images belonging to the same class tend to be close together in the embedding space. By storing the image embeddings from our training set in the vector database, we can classify a new image by searching for the most similar embeddings (nearest neighbors) and their associated labels. The most common label among these nearest neighbors is then predicted as the label for the new image. This is the basic principle behind how the [K-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm works.\n",
    "\n",
    "Using Milvus, a KNN Classifier is implemented with 'add' and 'predict' methods. This allows new examples and classes to be inserted in the database as needed and predictions to always take into account the latest samples in the database. \n",
    "\n",
    "To learn more about Milvus visit their [documentation page](https://milvus.io/docs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99288d41-da7b-4a2d-b45c-ada0b1d38c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "class KNeighborsClassifierMilvus:\n",
    "    def __init__(self, database=\"milvus_demo.db\", collection=\"knn\", rm=True, embedding_d=1536):\n",
    "        \n",
    "        self.database = database\n",
    "        self.collection = collection\n",
    "        self.id_tracker = 0 #track IDs to insert new data\n",
    "\n",
    "        #Delete local database if it exists\n",
    "        if rm:\n",
    "            Path.unlink(self.database, missing_ok=True)\n",
    "\n",
    "        #connect to database\n",
    "        self.client = MilvusClient(self.database)\n",
    "       \n",
    "        #setup collection \n",
    "        if not self.client.has_collection(collection_name=self.collection):\n",
    "            #create collection in database. This will associate a vector with the metadata\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection,\n",
    "                dimension=embedding_d, #1536 for NVDINOv2, 1024 for NVCLIP \n",
    "                metric_type=\"L2\"\n",
    "                )\n",
    "        \n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add labelled embeddings to classifier\"\"\"\n",
    "        milvus_samples = []\n",
    "        for i, vector in enumerate(x):\n",
    "            sample = {\"id\":self.id_tracker, \"vector\":vector, \"label\":y[i]}\n",
    "            self.id_tracker += 1\n",
    "            milvus_samples.append(sample)\n",
    "        self.client.insert(collection_name=self.collection, data=milvus_samples)\n",
    "    def predict(self, x, n_neighbors=1):\n",
    "        \"\"\"pass in 2d list of vectors\"\"\"\n",
    "        labels = []\n",
    "        results = self.client.search(collection_name=self.collection, data=x, limit=n_neighbors, output_fields=[\"label\"])\n",
    "        for result in results:\n",
    "            neighbor_labels = [x[\"entity\"][\"label\"] for x in result]\n",
    "            label_counter = Counter(neighbor_labels)\n",
    "            label = label_counter.most_common()[0][0] #get most common label from neighbors \n",
    "            labels.append(label)\n",
    "        return labels \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f222c7-5452-4346-a768-5024791d8590",
   "metadata": {},
   "source": [
    "Now we can run this and compare the results with the SKLearn models. The accuracy should be similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbe496-f0f8-4dc4-abdd-f81b0f4f2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifierMilvus(embedding_d=1536) #pass embedding_d=1024 if using NVCLIP embeddings\n",
    "model.add(x_train, y_train)\n",
    "y_predict = model.predict(x_test, n_neighbors=2)\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e10378-f073-40d5-8157-dacd71c11497",
   "metadata": {},
   "source": [
    "## Part 3.3 Interactive Gradio UI for Few Shot Classification\n",
    "\n",
    "This can all be put together to build a Gradio UI to quickly experiement with few shot classification. The UI allows you to add any number of classes and upload sample images and inference on new images. It can be launched to use NVDINOv2 or NVCLIP. You must manually stop the cell the bring down the Gradio UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936bf06-6b92-4d8e-8289-66f3240397aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{python_exe} main.py {api_key} nvdinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29132f4d-b9df-482f-9785-f4d71fb2dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{python_exe} main.py {api_key} nvclip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
